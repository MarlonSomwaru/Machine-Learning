{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYC Open Data currently publishes the [NYPD Motor Vehicle Collisions - Crashes](https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions-Crashes/h9gi-nx95) data set that breaks down every automotive collision in NYC.  The purpose of the data set is for the public to understand how dangerous certain intersections are as well as the causes and outcomes of accidents.  Some of the data is incomplete, especially the more recent data points. Let's take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "Crash_Data = pd.read_csv(\"NYPD_Motor_Vehicle_Collisions_-_Crashes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crash_Data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a data set of 1.59 million accidents and 29 variables concerning each. Let's take a closer look at what the 29 variables in the data set are:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for i in Crash_Data.columns:\n",
    "    print(\"|\"+i[0].upper() + i[1:].lower()+\"|  |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Variable   | Description   |\n",
    "|:-:|:-:|\n",
    "|Date|Occurrence date of collision  |\n",
    "|Time|Occurrence time of collision  |\n",
    "|Borough|Borough where collision occurred  |\n",
    "|Zip code|Postal code of incident occurrence  |\n",
    "|Latitude|Latitude coordinate  |\n",
    "|Longitude|Longitude coordinate  |\n",
    "|Location|Latitude, Longitude pair  |\n",
    "|On street name|Street on which the collision occurred  |\n",
    "|Cross street name|Nearest cross street to the collision  |\n",
    "|Off street name|Street address if known  |\n",
    "|Number of persons injured|Number of persons injured  |\n",
    "|Number of persons killed|Number of persons killed  |\n",
    "|Number of pedestrians injured|Number of pedestrians injured  |\n",
    "|Number of pedestrians killed|Number of pedestrians killed  |\n",
    "|Number of cyclist injured|Number of cyclists injured  |\n",
    "|Number of cyclist killed|Number of cyclists killed  |\n",
    "|Number of motorist injured|Number of vehicle occupants injured  |\n",
    "|Number of motorist killed|Number of vehicle occupants killed  |\n",
    "|Contributing factor vehicle 1|Factors contributing to the collision for designated vehicle  |\n",
    "|Contributing factor vehicle 2|Factors contributing to the collision for designated vehicle  |\n",
    "|Contributing factor vehicle 3|Factors contributing to the collision for designated vehicle  |\n",
    "|Contributing factor vehicle 4|Factors contributing to the collision for designated vehicle  |\n",
    "|Contributing factor vehicle 5|Factors contributing to the collision for designated vehicle  |\n",
    "|Collision_id|Unique record code generated by system. Primary Key  |\n",
    "|Vehicle type code 1|Type of vehicle based on the selected vehicle category  |\n",
    "|Vehicle type code 2|Type of vehicle based on the selected vehicle category  |\n",
    "|Vehicle type code 3|Type of vehicle based on the selected vehicle category  |\n",
    "|Vehicle type code 4|Type of vehicle based on the selected vehicle category  |\n",
    "|Vehicle type code 5|Type of vehicle based on the selected vehicle category  |\n",
    "\n",
    "The first thing we'll do is convert the `DATE` column to a type of datetime, which allow for easier data manipulation as we move on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Crash_Data['DATE']=Crash_Data['DATE'].map(lambda x: pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vehicle type codes include ATVs, bicycles, cars or SUVs, E-Bikes, E-Scooters, trucks or buses, motorcycles, and other.  Not sure what other methods of transport people have in NYC. Longboards? Electric skateboards?\n",
    "The descriptions in the table above came directly from NYC OpenData.  I have my doubts about the reports being consistent thorughout the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Crash_Data.groupby('VEHICLE TYPE CODE 1')['VEHICLE TYPE CODE 1'].count().sort_values(ascending=False)[0:30])\n",
    "print(\"\\nNumber of accidents above: \"+ str(sum(Crash_Data.groupby('VEHICLE TYPE CODE 1')['VEHICLE TYPE CODE 1'].count().sort_values(ascending=False)[0:30])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's a lot of in the \"other\" category.  It wouldn't be very practical to find every accident that involved a horse or a well driller.  There are actually 686 unique values in this column, so let's try to correct them in terms of frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The groups above make up 1.57 million of the accidents in our data set. For simplicity, we'll stick to these groups and attempt to narrow them down.  The groups we'll use are: \n",
    "* Car\n",
    "* SUV\n",
    "* Taxi\n",
    "* Bus\n",
    "* Commercial vehicle\n",
    "* Bicycle \n",
    "* Motorcycle\n",
    "* Pickup truck\n",
    "* Van\n",
    "* Commercial vehicle\n",
    "* Other\n",
    "* Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Replacement_Type = {\"PASSENGER VEHICLE\":\"Car\", \"SPORT UTILITY / STATION WAGON\": \"SUV\",\n",
    "                   \"Sedan\":\"Car\", \"Station Wagon/Sport Utility Vehicle\":\"SUV\", \"TAXI\": \"Taxi\",\"VAN\":\"Van\",\n",
    "                   \"OTHER\":\"Other\", \"PICK-UP TRUCK\": \"Pick-up Truck\", \"SMALL COM VEH(4 TIRES)\":\"Commercial Vehicle\",\n",
    "                   \"LARGE COM VEH(6 OR MORE TIRES)\":\"Commercial Vehicle\", \"BUS\":\"Bus\", \"LIVERY VEHICLE\":\"Taxi\",\n",
    "                   \"Box Truck\":\"Commercial Vehicle\",\"MOTORCYCLE\":\"Motorcycle\",\"BICYCLE\":\"Bicycle\",\"Bike\":\"Bicycle\",\n",
    "                   \"Tractor Truck Diesel\":\"Commercial Vehicle\",\"TK\":\"Commercial Vehicle\",\"BU\":\"Other\",\"Convertible\":\"Car\",\n",
    "                   \"Dump\":\"Other\",\"Ambulance\":\"Other\"}\n",
    "\n",
    "def Check_Replacement_Type(replacement_dict, current_value):\n",
    "    try:\n",
    "        if current_value in replacement_dict:\n",
    "            return replacement_dict.get(current_value)\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "Crash_Data['VEHICLE TYPE CODE 1'] = Crash_Data['VEHICLE TYPE CODE 1'].apply(lambda x: Check_Replacement_Type(Replacement_Type, x))\n",
    "\n",
    "Crash_Data['VEHICLE TYPE CODE 1'] = Crash_Data['VEHICLE TYPE CODE 1'].dropna()\n",
    "\n",
    "Crash_Data = Crash_Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we defined a dictionary containing a list of substitutions for the original vehicle types.  Some of the types repeat but come up as unique due to spelling differences.\n",
    "\n",
    "The function `Check_Replacement_Type` is used to compare the values in the dateframe to those in the dictionary.  It's much faster to create a function and apply it as opposed to just looping over the array.  Anything that wasn't in the dictionary was then dropped, leaving us with about 1.49 million accidents, which is more than enough.  The last line resets the index of the dateframe to account for the dropped values.\n",
    "\n",
    "Looking at the same column again, things seem a bit more orderly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(Crash_Data.groupby('VEHICLE TYPE CODE 1')['VEHICLE TYPE CODE 1'].count().sort_values(ascending=False))\n",
    "print(\"\\nNumber of accidents with 1 vehicle: \"+ str(sum(Crash_Data.groupby('VEHICLE TYPE CODE 1')['VEHICLE TYPE CODE 1'].count().sort_values(ascending=False))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll repeat a similar method for the other vehicle code columns, without dropping them.  Not every accident will have two or more cars, but it would be interesting to look it if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2,6):\n",
    "    Crash_Data['VEHICLE TYPE CODE ' + str(i)] = Crash_Data['VEHICLE TYPE CODE ' + str(i)].apply(lambda x: Check_Replacement_Type(Replacement_Type, x))\n",
    "    print(Crash_Data.groupby('VEHICLE TYPE CODE ' + str(i))['VEHICLE TYPE CODE ' + str(i)].count().sort_values(ascending=False))\n",
    "    print(\"\\nNumber of accidents with \" + str(i) + \" vehicles: \"+ str(sum(Crash_Data.groupby('VEHICLE TYPE CODE '+str(i))['VEHICLE TYPE CODE '+str(i)].count().sort_values(ascending=False))))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plot1 = sns.countplot(x='VEHICLE TYPE CODE 1', data=Crash_Data, hue=Crash_Data['VEHICLE TYPE CODE 1'],dodge=False)\n",
    "plot1.set_title(\"Accidents with 1 Vehicle\")\n",
    "plot1.set_ylabel(\"Number of accidents per vehicle type\")\n",
    "plot1.set_xlabel(\"Vehicle Type\")\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(nrows=3,ncols=2,figsize=(10,5))\n",
    "\n",
    "Vehicles_Count_Dict = {1:\"One Vehicle\", 2:\"Two Vehicles\", 3:\"Three Vehicles\",\n",
    "                      4:\"Four Vehicles\", 5:\"Five Vehicles\"}\n",
    "VCount = 0\n",
    "count = 1\n",
    "for i in range(1,6):\n",
    "    \n",
    "    VCount+=1\n",
    "    plt.subplot(5,1,i)\n",
    "    sns.countplot(x='VEHICLE TYPE CODE '+str(i), data=Crash_Data,hue=Crash_Data['VEHICLE TYPE CODE '+str(i)],dodge=False)\n",
    "    plt.xticks([])\n",
    "    plt.xlabel(\"Vehicle Types\",fontsize=10)\n",
    "    plt.ylabel(\"Number of Accidents\",fontsize=10)\n",
    "    plt.title(\"Accidents involving \"+Vehicles_Count_Dict.get(i),fontsize=15)\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc=2)\n",
    "    \n",
    "    count+=1\n",
    "    if count == 2: count = 1\n",
    "    plt.figure(figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='VEHICLE TYPE CODE 1', data=Crash_Data,hue=Crash_Data['VEHICLE TYPE CODE 1'],dodge=False)\n",
    "plt.xticks([])\n",
    "plt.xlabel(\"Vehicle Types\",fontsize=10)\n",
    "plt.ylabel(\"Number of Accidents\",fontsize=10)\n",
    "plt.title(\"Accidents involving One Vehicle\",fontsize=15)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the accidents happening the most? A simple bar chart shows the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Crash_Data.groupby('BOROUGH')['BOROUGH'].count().sort_values(ascending=False))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plot2 = sns.countplot(x=\"BOROUGH\", data=Crash_Data)\n",
    "plot2.set_title(\"Accidents by Borough\")\n",
    "plot2.set_ylabel(\"Number of accidents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like Brooklyn has the highest frequency, but we'll come back to geographic distribution of accidents later.\n",
    "Now let's look at the most common reasons accidents occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Crash_Data.groupby('CONTRIBUTING FACTOR VEHICLE 1')['CONTRIBUTING FACTOR VEHICLE 1'].count().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of cleanliness of the data, this column is much better compared to the vehicle type column.  A similar approach will be taken in terms of grouping, but the rows won't be dropped if blank.  The accident occurred, so it should be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    Crash_Data['CONTRIBUTING FACTOR VEHICLE ' + str(i)] = Crash_Data['CONTRIBUTING FACTOR VEHICLE ' + str(i)].apply(lambda x: str(x).lower())\n",
    "    Crash_Data['CONTRIBUTING FACTOR VEHICLE ' + str(i)] = Crash_Data['CONTRIBUTING FACTOR VEHICLE ' + str(i)].replace(['80','1'],'unspecified')\n",
    "    Crash_Data['CONTRIBUTING FACTOR VEHICLE ' + str(i)] = Crash_Data['CONTRIBUTING FACTOR VEHICLE ' + str(i)].replace(['illnes'],'illness')\n",
    "\n",
    "print(Crash_Data.groupby('CONTRIBUTING FACTOR VEHICLE 1')['CONTRIBUTING FACTOR VEHICLE 1'].count().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though slightly different than the approach used for replacing vehicle types, using numpy's built in `replace` method is pretty fast as well. It's also easier than making a dictionary.\n",
    "\n",
    "Instead of just looking at the data in terms of its variables, it can also be analyzed as a time series.  First, we'll create a seperate data frame from handling the time series data and the plots associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "Crash_Data_By_Date = pd.DataFrame()\n",
    "\n",
    "#Crash_Data_By_Date[\"Date\"] = Crash_Data['DATE']\n",
    "Crash_Data_By_Date[\"Date\"] = Crash_Data['DATE'].map(lambda x: x)\n",
    "\n",
    "#Crash_Data_By_Date['Time'] = Crash_Data['TIME']\n",
    "Crash_Data_By_Date['Time'] = Crash_Data['TIME'].map(lambda x: x)\n",
    "\n",
    "#Crash_Data_By_Date['Date'] = pd.to_datetime(Crash_Data_By_Date['Date'])\n",
    "Crash_Data_By_Date['Date'] = Crash_Data_By_Date['Date'].map(lambda x: pd.to_datetime(x))\n",
    "\n",
    "Crash_Data_By_Date['Day of Week'] = Crash_Data_By_Date['Date'].dt.day_name()\n",
    "\n",
    "Crash_Data_By_Date['Month'] = Crash_Data_By_Date['Date'].dt.month\n",
    "\n",
    "Crash_Data_By_Date['Day'] = Crash_Data_By_Date['Date'].dt.day\n",
    "Crash_Data_By_Date['Year'] = Crash_Data_By_Date['Date'].dt.year\n",
    "\n",
    "Crash_Data_By_Date = Crash_Data_By_Date.sort_values(by=['Date'])\n",
    "\n",
    "Crash_Data_By_Date = Crash_Data_By_Date.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Crash_Data_By_Date.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first plot will show us the distribution of all of the accidents over our time period.  The distribution is just about symmetric considering how close together the mean and median are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Accidents = Crash_Data_By_Date['Date'].value_counts()\n",
    "\n",
    "plt.figure(1, figsize=(8,8))\n",
    "sns.distplot(Total_Accidents.values, color='r')\n",
    "plt.title(\"Distribution of All Accidents Per Day\",fontsize=15)\n",
    "plt.text(820, 0.0045,'Mean: '+ str( \"{:.{}f}\".format(Total_Accidents.mean(), 2 ))+\" accidents\", fontsize=10)\n",
    "plt.text(820, 0.0043,'Median: '+ str( \"{:.{}f}\".format(Total_Accidents.median(), 2 ))+\" accidents\", fontsize=10)\n",
    "plt.text(820, 0.0041,'Standard Dev: '+ str( \"{:.{}f}\".format(Total_Accidents.std(), 2 ))+\" accidents\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll look at the distrubution of crashes over the 8 years in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(16,16))\n",
    "count = 0\n",
    "Color_Count = 0\n",
    "\n",
    "Colors = {1:'#4e876f', 2:'#298378', 3:'#007d85', 4:'#007692', 5:'#006d9b', 6:'#00629f',\n",
    "          7: '#38549a', 8: '#5e428c'}\n",
    "\n",
    "for i, col in enumerate(Crash_Data_By_Date[\"Year\"].unique()):\n",
    "    Color_Count+=1\n",
    "    \n",
    "    Crash_Data_By_Year = Crash_Data_By_Date[\"Date\"].loc[Crash_Data_By_Date[\"Year\"]==col].value_counts()\n",
    "    \n",
    "    ax[i//4,count].set_ylim([0.0,0.007])\n",
    "    sns.distplot(Crash_Data_By_Year.values,  ax=ax[i//4,count], color=Colors.get(Color_Count))\n",
    "    ax[i//4,count].title.set_text(\"Year:\"+str(col))\n",
    "    ax[i//4,count].text(265, 0.0065,'Total:'+ str( \"{:.{}f}\".format( Crash_Data_By_Year.sum(), 0 )), fontsize=10)\n",
    "    ax[i//4,count].text(265, 0.0062,'Mean:'+ str( \"{:.{}f}\".format( Crash_Data_By_Year.mean(), 2 )), fontsize=10)\n",
    "    ax[i//4,count].text(265, 0.0059,'Median:'+ str( \"{:.{}f}\".format( Crash_Data_By_Year.median(), 2 )), fontsize=10)\n",
    "    ax[i//4,count].text(265, 0.0056,'STD:'+ str( \"{:.{}f}\".format( Crash_Data_By_Year.std(), 2 )), fontsize=10)\n",
    "    \n",
    "    \n",
    "    count+=1\n",
    "    if count == 4: count = 0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr =  Crash_Data.corr()\n",
    "plt.subplots(figsize=(20,9))\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmaps\n",
    "API_KEY =\"AIzaSyAFHEsR06LzmfjYNuLP83mymoc49L4eeL0\"\n",
    "gmaps.configure(api_key=\"AIzaSyAFHEsR06LzmfjYNuLP83mymoc49L4eeL0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crash_2019 = Crash_Data[[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crash_Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crash_2019 = Crash_Data[Crash_Data['DATE'].dt.year==2019]\n",
    "Crash_2019=Crash_2019.dropna(subset=['LATITUDE'])\n",
    "Crash_2019=Crash_2019.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Crash_2019['LOCATION'][0]\n",
    "test =test.replace('POINT ','')\n",
    "test = test.replace(' ',',')\n",
    "eval(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f02cb3534cc40f1b6f534c854959ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(layout=FigureLayout(height='420px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = gmaps.figure()\n",
    "#heatmap_layer = gmaps.heatmap_layer(eval(test),\n",
    " #                                   max_intensity=30,point_radius=5)\n",
    "#fig.add_layer(heatmap_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb56dbdea25f48ca9585e45273f17f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(layout=FigureLayout(height='420px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = gmaps.figure(map_type='SATELLITE')\n",
    "\n",
    "# generate some (latitude, longitude) pairs\n",
    "locations = [(51.5, 0.1), (51.7, 0.2), (51.4, -0.2), (51.49, 0.1)]\n",
    "\n",
    "heatmap_layer = gmaps.heatmap_layer(locations)\n",
    "fig.add_layer(heatmap_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter nbextension enable --py gmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_york_coordinates = (40.75, -74.00)\n",
    "gmaps.figure(center=new_york_coordinates, zoom_level=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
