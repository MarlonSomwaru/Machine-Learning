{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### An introduction to Support Vector Machines with a Python example focused on trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as any other machine learning algorithm does, a support vector machine (SVM) takes data as input, attempts to find & recognize patterns, and then tells us what it learned.  Support vector machines fall into the category of supervised learning, which means that it creates a function that will map a given input to an output.  More specifically, a SVM is a classification algorithm.\n",
    "\n",
    "Before we can start implementing trading algorithms and seeking alpha, let's figure out how an SVM works\n",
    "#### Maximal Margin Classifier\n",
    "The support vector machine algorithm comes from the maximial margin classifier.  The **maximal margin classifier** uses the distance from a given decision boundary to classify an input.  The greater the distance, or *margin* , the better the classifier is at handling the data. On a catesian plane, the boundary can be thought of as a line.  In three dimensional space, it is a plane, but after than it becomes hard to conceptualize. This boundary can be better thought of as a **hyperplane**, specifically one of dimension $p-1$, where $p$ is the dimension of the data point.\n",
    "Our boundary, or hyperplane, is known as a seperating hyperplane, because it is used to seperate the data points into desired categories. In general, there are many hyperplanes that can seperate a given data set, but the one we care about is the *maximal margin hyperplane* or the *optimal separating hyperplane*.  This separating hyperplane is the one with the largest minimum distance from each data point in the training set.  By using this hyperplane to classify a data point from the test set, we have the maximal margin classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "points=np.array(np.random.random((100,3)))\n",
    "\n",
    "plt.scatter(points[:,0], points[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the maximal margin classifier works, to a degree.  If you have a data set which cannot be separated by a hyperplane, you can no longer use this.  Sometimes, you may run into a data that has more than two categories, which makes a linear boundary useless.\n",
    "\n",
    "[](svm meme 1.jpg)\n",
    "\n",
    "At this point, you have to consider you options.  \n",
    "1. You can base your classifier on the seperating hyperplane as explained earlier.  But the hyperplane doesn't exist...so you have no classifier.\n",
    "2. Consider a classifier that isn't perfect, but it can work some/most of the time\n",
    "\n",
    "#### Support Vector Classifier\n",
    "I like the second option too.  By using a classifier that isn't perfect, you can at least handle most observations, and introduce a level of adaptation to the model when it is presented with new data.\n",
    "\n",
    "This evolution of the maximal margin classifier is known as the **support vector classifier** (SVC), or the soft margin classifier.  Instead of being exact and not very robust in its classification, the SVC allows some observations to be on the wrong side of the margin and/or hyperplane (where the soft comes from), for the sake of getting classification mostly correct.\n",
    "\n",
    "Without getting into too much math, the algorithm determines which side of the hyperplane an observation will lie on by finding a solution to an optimization problem that uses a tuning parameter, the width of the margin (which it tries to maximize) and slack variables.\n",
    "\n",
    "The tuning parameter is used to control the bias-variance tradeoff.  When it is small, the classifier fits the data well as the margins are small.  In other words, low bias, high variance.  A larger tuning parameter is the opposite.  It allows for more observations to be on the wrong side of the margin allowing for high bias and low variance.\n",
    "\n",
    "Slack variables in particular are pretty cool.  They allow data points to be on the side of the margin or hyperplane.  They are also used to transform inequalities into equalities.  The values that the slack values take on can also tell us about the behavior of a given data point. If the slack variable for a given data point is equal to 0, then that data point is on the right side of the margin.  If the slack variable is greater than 0 but less than 1, the data point is on the wrong side of the margin, but on the right side of the hyperplane.  If the slack variable is greater than 1, the data point is on the wrong side of the hyper plane.\n",
    "\n",
    "The main reason this optimization matters is its affect on the hyperplane. The only values that affect the hyperplane, and in turn how data points are classified, are those that are on the margin, or on the wrong side of it.  If an object is on the right side of the hyperplane, it has not affect on it.  The classifer gets its name from the former data points, as they are known as **support vectors**.\n",
    "\n",
    "#### Finally, Support Vector Machines\n",
    "The support vector machines builds on the optimization in support vector classifiers by growing the feature space by using **kernels**. \n",
    "\n",
    "Kernels, similar to the previous optimization, uses a fair bit of math.  Put simply, kernels tell us how similar data points are.  By assigning weights to to sequences of data, it can identify how similar two points are, given that it has learned how to compare them.  Kernels allow data to be processed in simpler terms, as opposed to being done in a higher dimensional space.  More specifically, it computes inner products between all possible outputs of all of the pairs of data points in the feature space.  By using kernels instead of enlarging the feature space, the algorithm can be much more efficient.  It uses one function to compare pairs of distinct data points as opposed to using functions for original features in the data set.\n",
    "\n",
    "Many different kernels exist including the RBF kernel, graph kernels, the linear kernel, polynomial kernel.  For example, the linear kernel compares a pair of data points by using their bivariate correlation.  The polynomial kernel attempts to fit an SVC in a higher dimensional space. A support vector classifier is the same as using an SVM with a polynomial kernel of degree 1.\n",
    "\n",
    "Basically, the main goal of the Support Vector Machine is to construct a hyperplane, which it then uses to classify data.  Despite generally being categorized as a classification algorithm, there is an extension of the Support Vector Machine used for regression, known as *Support Vector Regression*.\n",
    "\n",
    "#### Support Vector Machines for Trading\n",
    "\n",
    "Before I get into this application, know that this is by no means advice on how/what you should trade.  That's on you.\n",
    "\n",
    "We'll start by gathering our data.\n",
    "\n",
    "We'll use a time period going back about five years, October 28, 2014 to October 28, 2019.  The stocks that we will get price data for are the components of the Dow Jones Industrial Average.\n",
    "\n",
    "Yahoo Finance used to be really easy to get data from, but most packages no longer work, so we'll also create a web scraper in the process.\n",
    "\n",
    "The first things we'll do is import all of the packages we'll need an then use the request package to scrape the contents of [this](https://finance.yahoo.com/quote/%5EDJI/components?p=%5EDJI) on Yahoo Finance.  This page contains the names of all of the companies that make up the Dow Jones Industrial Average, as well as their tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "Dow_Page = requests.get('https://finance.yahoo.com/quote/%5EDJI/components?p=%5EDJI')\n",
    "Dow_Content = Dow_Page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use BeautifulSoup4 to make the information in `Dow_Content` searchable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(Dow_Content)\n",
    "\n",
    "data = list(soup.findAll(\"td\",{\"class\":\"Py(10px) Ta(start) Pend(10px)\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lines above parse the data gathered from the webpage and search for the bit of HTML code that corresponds to the table on the page.  This can be found by right click on the area of the page, inspecting the element, and with a little investigation you can find the class name used above.\n",
    "\n",
    "There will be two types of lines that the search will come across:\n",
    "1. Lines containing the ticker\n",
    "2. Lines containing the company name with no ticker\n",
    "\n",
    "We don't care for the later, so when the loop finds them, it ignores them and moves on.  A few string operations to trim the extra fat and we have our ticker.  Each ticker is then added to a list for safe keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ticker_List = []\n",
    "for i in data:\n",
    "    TempData = str(i)\n",
    "    if \"title\" in TempData:\n",
    "        TempData = TempData[TempData.find(\"title\"):]\n",
    "        TempData = TempData[TempData.find(\">\")+1:TempData.find(\"<\")]\n",
    "        Ticker_List.append(TempData)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yahoo Finance uses a Unix time stamp in their url, so we make use of the `time` package to convert our start and end dates to the desired format.  It can take either `struct_time` (more about that [here](https://docs.python.org/2/library/time.html#time.struct_time)) or a tuple of 9 time arguments.  We don't really care for anything past the date here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Start_Date = int(time.mktime((2014,10,28,4,0,0,0,0,0)))\n",
    "End_Date = int(time.mktime((2019,10,28,4,0,0,0,0,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Stock_Data` dictionary will hold our parsed data.  The keys in the dictionary will be the ticker of a given stock.  For each stock, the function `ScrapeYahoo` will create a dateframe containing open, high, low, close, and volume data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock_Data = {}\n",
    "\n",
    "for i in Ticker_List:\n",
    "    ScrapeYahoo(i, Start_Date, End_Date)\n",
    "    print(i + \" done\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ScrapeYahoo` function takes three arguments:\n",
    "1. ticker, a string representing a given stock\n",
    "2. start, a unix timestamp representing the start date \n",
    "3. end, a unix timestamp representing the current day\n",
    "\n",
    "It combines these with the base url for Yahoo Finance and gets the data from the desired web page.  Instead of processing it like we did earlier, we parse the JSON data from the page.  Yahoo Finance uses cookies now, and simply using the HTML code will throw an error.\n",
    "\n",
    "The lines after parse the content of the JSON data.  Something that helped a lot while I was initially exploring the dataset was the `keys()` method for Python dictionaries.  It made traversing the JSON data much easier.  You can read about it [here](https://www.programiz.com/python-programming/methods/dictionary/keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeYahoo(ticker, start, end):\n",
    "    \n",
    "    #Form the URL to be scraped\n",
    "    Base_Url = 'https://query1.finance.yahoo.com/v8/finance/chart/'\n",
    "    Scrape_Url = base_url + ticker + \"?period1=\" + str(start)+\"&period2=\"+str(end)+\"&interval=1d\"\n",
    "    \n",
    "    #Get data from page\n",
    "    r = requests.get(Scrape_Url)\n",
    "    Page_Data = r.json()\n",
    "    \n",
    "    # Compile data into a DataFrame\n",
    "    Stock_df = pd.DataFrame()\n",
    "    Stock_df['DateTime'] = Page_Data['chart']['result'][0]['timestamp']\n",
    "    Stock_df['DateTime'] = Stock_df['DateTime'].apply(lambda x: datetime.datetime.fromtimestamp(x).date().isoformat())\n",
    "    Stock_df[\"Open\"] = Page_Data[\"chart\"][\"result\"][0][\"indicators\"][\"quote\"][0][\"open\"]\n",
    "    Stock_df[\"High\"] = Page_Data[\"chart\"][\"result\"][0][\"indicators\"][\"quote\"][0][\"high\"]\n",
    "    Stock_df[\"Low\"] = Page_Data[\"chart\"][\"result\"][0][\"indicators\"][\"quote\"][0][\"low\"]\n",
    "    Stock_df[\"Close\"] = Page_Data[\"chart\"][\"result\"][0][\"indicators\"][\"quote\"][0][\"close\"]\n",
    "    Stock_df[\"Volume\"] = Page_Data[\"chart\"][\"result\"][0][\"indicators\"][\"quote\"][0][\"volume\"]\n",
    "    Stock_df = Stock_df.set_index(\"DateTime\")\n",
    "    \n",
    "    #Add data to a dictionary containing all values\n",
    "    Stock_Data[ticker] =  Stock_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have historical price data, now what?  Recall that the support vector machine is a classification algorithm.  We're going to attempt to classify price movements in to *buy* and *sell* signals with the help of technical analysis.\n",
    "\n",
    "Technical analysis is a methodology that uses past data to forecast the future direction of price.  In general, technical indicators use price data and volume in their calculations. The motivation for the indicators chosen come from the papers listed in the references section at the end of the article.\n",
    "\n",
    "One very important thing to pay attention to before moving on: **look-ahead bias**.\n",
    "We already have all of the closing data, which is what will be used for calculations.  In a real world scenario, the most you have is the previous day's closing.  We have to make sure our calculations don't take in data that technically had not occurred yet.\n",
    "To do this, we will *lag* the data. That is, shift our data back one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib as ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of the `talib` library perform the technical analysis calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-10-28</th>\n",
       "      <td>104.959999</td>\n",
       "      <td>106.519997</td>\n",
       "      <td>104.860001</td>\n",
       "      <td>106.269997</td>\n",
       "      <td>4173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-29</th>\n",
       "      <td>106.720001</td>\n",
       "      <td>107.269997</td>\n",
       "      <td>105.419998</td>\n",
       "      <td>105.849998</td>\n",
       "      <td>4060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-30</th>\n",
       "      <td>105.209999</td>\n",
       "      <td>106.589996</td>\n",
       "      <td>104.699997</td>\n",
       "      <td>106.339996</td>\n",
       "      <td>2890500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-31</th>\n",
       "      <td>107.830002</td>\n",
       "      <td>107.949997</td>\n",
       "      <td>106.980003</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>4461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11-03</th>\n",
       "      <td>107.419998</td>\n",
       "      <td>107.500000</td>\n",
       "      <td>106.029999</td>\n",
       "      <td>106.300003</td>\n",
       "      <td>4587600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Volume\n",
       "DateTime                                                           \n",
       "2014-10-28  104.959999  106.519997  104.860001  106.269997  4173500\n",
       "2014-10-29  106.720001  107.269997  105.419998  105.849998  4060700\n",
       "2014-10-30  105.209999  106.589996  104.699997  106.339996  2890500\n",
       "2014-10-31  107.830002  107.949997  106.980003  107.000000  4461100\n",
       "2014-11-03  107.419998  107.500000  106.029999  106.300003  4587600"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stock_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "https://pdfs.semanticscholar.org/4d9f/4d308e318eb65f02bd12d2abc37ce7493698.pdf\n",
    "https://doi.org/10.1016/j.jfds.2018.04.003\n",
    "https://blog.quantinsti.com/trading-using-machine-learning-python-svm-support-vector-machine/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'https://query1.finance.yahoo.com/v7/finance/download/UTX?period1=1414468800&period2=1572235200&interval=1d&events=history&crumb=L8FTcMr2/d2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTX done\n",
      "MRK done\n",
      "NKE done\n",
      "WMT done\n",
      "CVX done\n",
      "PG done\n",
      "CAT done\n",
      "AXP done\n",
      "DIS done\n",
      "BA done\n",
      "VZ done\n",
      "KO done\n",
      "JPM done\n",
      "IBM done\n",
      "INTC done\n",
      "CSCO done\n",
      "JNJ done\n",
      "WBA done\n",
      "TRV done\n",
      "UNH done\n",
      "XOM done\n",
      "AAPL done\n",
      "HD done\n",
      "V done\n",
      "PFE done\n",
      "DOW done\n",
      "MCD done\n",
      "GS done\n",
      "MMM done\n",
      "MSFT done\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "Dow_Page = requests.get('https://finance.yahoo.com/quote/%5EDJI/components?p=%5EDJI')\n",
    "Dow_Content = Dow_Page.content\n",
    "\n",
    "soup = BeautifulSoup(Dow_Content)\n",
    "\n",
    "data = list(soup.findAll(\"td\",{\"class\":\"Py(10px) Ta(start) Pend(10px)\"}))\n",
    "\n",
    "Ticker_List = []\n",
    "for i in data:\n",
    "    TempData = str(i)\n",
    "    if \"title\" in TempData:\n",
    "        TempData = TempData[TempData.find(\"title\"):]\n",
    "        TempData = TempData[TempData.find(\">\")+1:TempData.find(\"<\")]\n",
    "        Ticker_List.append(TempData)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "Start_Date = int(time.mktime((2014,10,28,4,0,0,0,0,0)))\n",
    "End_Date = int(time.mktime((2019,10,28,4,0,0,0,0,0)))\n",
    "\n",
    "Stock_Data = {}\n",
    "\n",
    "for i in Ticker_List:\n",
    "    ScrapeYahoo(i, Start_Date, End_Date)\n",
    "    print(i + \" done\")\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
